# Legal Mediation System - Architecture Overview

This document provides a comprehensive overview of the Proposer platform architecture, showing how the frontend (Next.js), backend (FastAPI), and core packages (RAG, KG, LLM) work together.

## System Architecture

```mermaid
graph TB
    %% Frontend Layer
    subgraph Frontend["üåê Frontend (Next.js 16 App Router)"]
        HomePage["Home Page<br/>/"]
        ChatListPage["Chat List<br/>/chat"]
        ChatSessionPage["Chat Session<br/>/chat/[sessionId]"]
        PredictionPage["Prediction View<br/>/prediction/[caseId]"]
        AdminPage["Admin<br/>/admin"]
        
        subgraph Components["React Components"]
            ChatContainer["ChatContainer"]
            IntakeSidebar["IntakeSidebar"]
            MessageList["MessageList"]
            PredictionViewer["PredictionViewer"]
            ReasoningTrace["ReasoningTrace"]
        end
        
        subgraph APIClients["API Client Layer"]
            ChatAPIClient["chatApi"]
            PredictionsAPIClient["predictionsApi"]
            APIClient["api (fetch wrapper)"]
        end
    end

    %% API Layer
    subgraph Backend["‚öôÔ∏è Backend API (FastAPI)"]
        MainApp["main.py<br/>FastAPI App"]
        
        subgraph Routers["API Routers (Endpoints)"]
            ChatRouter["/chat<br/>‚Ä¢ POST /start<br/>‚Ä¢ POST /message<br/>‚Ä¢ POST /set-role<br/>‚Ä¢ GET /session/:id<br/>‚Ä¢ DELETE /session/:id<br/>‚Ä¢ GET /sessions"]
            PredictionsRouter["/predictions<br/>‚Ä¢ POST /generate<br/>‚Ä¢ GET /:id<br/>‚Ä¢ GET /case/:caseId"]
            DisputesRouter["/disputes<br/>‚Ä¢ POST /create<br/>‚Ä¢ POST /validate-invite<br/>‚Ä¢ POST /join<br/>‚Ä¢ GET /:id"]
            EvidenceRouter["/evidence<br/>‚Ä¢ POST /upload/:caseId<br/>‚Ä¢ GET /:caseId<br/>‚Ä¢ DELETE /:evidenceId"]
            CasesRouter["/cases<br/>‚Ä¢ GET /:caseId<br/>‚Ä¢ GET /"]
        end
        
        subgraph Services["Service Layer"]
            IntakeService["IntakeService<br/>‚Ä¢ Session Management<br/>‚Ä¢ Conversation Flow<br/>‚Ä¢ Case File Building"]
            PredictionService["PredictionService<br/>‚Ä¢ Prediction Generation<br/>‚Ä¢ RAG Integration<br/>‚Ä¢ KG Integration"]
            DisputeService["DisputeService<br/>‚Ä¢ Dispute Creation<br/>‚Ä¢ Invite Codes<br/>‚Ä¢ Party Linking"]
            StorageService["StorageService<br/>‚Ä¢ File Uploads<br/>‚Ä¢ Supabase Integration<br/>‚Ä¢ Evidence Processing"]
        end
    end

    %% Core Packages Layer
    subgraph Packages["üì¶ Core Packages (Python)"]
        subgraph LLMOrchestrator["llm_orchestrator/"]
            IntakeAgent["IntakeAgent<br/>‚Ä¢ Dynamic Questioning<br/>‚Ä¢ Context Awareness<br/>‚Ä¢ Completeness Tracking"]
            PredictionAgent["PredictionEngine<br/>‚Ä¢ Prediction Synthesis<br/>‚Ä¢ Reasoning Trace<br/>‚Ä¢ Citation Generation"]
            ClaudeClient["ClaudeClient<br/>‚Ä¢ Anthropic API<br/>‚Ä¢ Structured Outputs<br/>‚Ä¢ Error Handling"]
            FactExtractor["FactExtractor<br/>‚Ä¢ Entity Extraction<br/>‚Ä¢ Evidence Processing"]
        end
        
        subgraph KGBuilder["kg_builder/"]
            GraphBuilder["GraphBuilder<br/>‚Ä¢ Node Creation<br/>‚Ä¢ Edge Validation<br/>‚Ä¢ Constraint Checking"]
            KGModels["KG Models<br/>‚Ä¢ Nodes (Party, Evidence, Issue)<br/>‚Ä¢ Edges (Supports, OccurredBefore)<br/>‚Ä¢ Validators"]
            JSONStore["JSONGraphStore<br/>‚Ä¢ Persistence<br/>‚Ä¢ Retrieval"]
        end
        
        subgraph RAGEngine["rag_engine/"]
            RAGPipeline["RAGPipeline<br/>‚Ä¢ Query Processing<br/>‚Ä¢ Orchestration"]
            HybridRetriever["HybridRetriever<br/>‚Ä¢ BM25 (keyword)<br/>‚Ä¢ Semantic Search<br/>‚Ä¢ Weighted Fusion"]
            ChromaStore["ChromaStore<br/>‚Ä¢ Vector DB<br/>‚Ä¢ Embeddings"]
            Reranker["Reranker<br/>‚Ä¢ Relevance Scoring<br/>‚Ä¢ Context Filtering"]
            LegalChunker["LegalChunker<br/>‚Ä¢ Section Detection<br/>‚Ä¢ Smart Chunking"]
        end
    end

    %% Data Layer
    subgraph DataLayer["üíæ Data & Storage"]
        subgraph FileSystem["Local File System"]
            SessionsDir["data/sessions/<br/>Session JSON files"]
            KGDir["data/knowledge_graphs/<br/>KG JSON files"]
            PredictionsDir["data/predictions/<br/>Prediction JSON files"]
            DisputesDir["data/disputes/<br/>Dispute JSON files"]
        end
        
        subgraph VectorDB["Vector Database"]
            ChromaDB["ChromaDB<br/>‚Ä¢ Embeddings<br/>‚Ä¢ BM25 Index<br/>‚Ä¢ data/embeddings/"]
        end
        
        subgraph ExternalStorage["External Storage"]
            Supabase["Supabase<br/>‚Ä¢ PostgreSQL (Auth, Metadata)<br/>‚Ä¢ Storage Buckets (Evidence)"]
        end
        
        subgraph CaseData["Case Data"]
            TribunalCases["Tribunal Decisions<br/>data/raw/bailii/<br/>~500+ PDFs"]
        end
    end

    %% External Services
    subgraph External["‚òÅÔ∏è External Services"]
        Anthropic["Anthropic API<br/>Claude 3.5 Sonnet/Haiku"]
        OpenAI["OpenAI API<br/>text-embedding-3-large"]
    end

    %% Frontend Connections
    ChatSessionPage --> ChatContainer
    ChatSessionPage --> IntakeSidebar
    PredictionPage --> PredictionViewer
    PredictionViewer --> ReasoningTrace
    
    ChatContainer --> ChatAPIClient
    IntakeSidebar --> ChatAPIClient
    PredictionViewer --> PredictionsAPIClient
    
    ChatAPIClient --> APIClient
    PredictionsAPIClient --> APIClient
    APIClient -->|"HTTP/JSON"| MainApp

    %% Backend Router to Service Connections
    MainApp --> ChatRouter
    MainApp --> PredictionsRouter
    MainApp --> DisputesRouter
    MainApp --> EvidenceRouter
    MainApp --> CasesRouter
    
    ChatRouter --> IntakeService
    ChatRouter --> DisputeService
    PredictionsRouter --> PredictionService
    DisputesRouter --> DisputeService
    EvidenceRouter --> StorageService
    CasesRouter --> IntakeService

    %% Service to Package Connections
    IntakeService --> IntakeAgent
    IntakeService --> FactExtractor
    IntakeService --> GraphBuilder
    
    PredictionService --> PredictionAgent
    PredictionService --> RAGPipeline
    PredictionService --> GraphBuilder
    
    DisputeService --> GraphBuilder

    %% LLM Orchestrator Internal Connections
    IntakeAgent --> ClaudeClient
    PredictionAgent --> ClaudeClient
    PredictionAgent --> RAGPipeline
    
    %% RAG Engine Internal Connections
    RAGPipeline --> HybridRetriever
    RAGPipeline --> Reranker
    HybridRetriever --> ChromaStore
    
    %% KG Builder Connections
    GraphBuilder --> KGModels
    GraphBuilder --> JSONStore

    %% Data Persistence Connections
    IntakeService -.->|"Save/Load"| SessionsDir
    DisputeService -.->|"Save/Load"| DisputesDir
    PredictionService -.->|"Save/Load"| PredictionsDir
    GraphBuilder -.->|"Save/Load"| KGDir
    ChromaStore -.->|"Read/Write"| ChromaDB
    HybridRetriever -.->|"Query"| ChromaDB
    StorageService -.->|"Upload"| Supabase
    RAGPipeline -.->|"Load"| TribunalCases

    %% External Service Connections
    ClaudeClient -->|"API Calls"| Anthropic
    ChromaStore -->|"Generate Embeddings"| OpenAI

    %% Styling
    classDef frontend fill:#3b82f6,stroke:#1e40af,color:#fff
    classDef backend fill:#8b5cf6,stroke:#6d28d9,color:#fff
    classDef package fill:#10b981,stroke:#059669,color:#fff
    classDef data fill:#f59e0b,stroke:#d97706,color:#fff
    classDef external fill:#ef4444,stroke:#dc2626,color:#fff
    
    class HomePage,ChatListPage,ChatSessionPage,PredictionPage,AdminPage,ChatContainer,IntakeSidebar,MessageList,PredictionViewer,ReasoningTrace,ChatAPIClient,PredictionsAPIClient,APIClient frontend
    class MainApp,ChatRouter,PredictionsRouter,DisputesRouter,EvidenceRouter,CasesRouter,IntakeService,PredictionService,DisputeService,StorageService backend
    class IntakeAgent,PredictionAgent,ClaudeClient,FactExtractor,GraphBuilder,KGModels,JSONStore,RAGPipeline,HybridRetriever,ChromaStore,Reranker,LegalChunker package
    class SessionsDir,KGDir,PredictionsDir,DisputesDir,ChromaDB,Supabase,TribunalCases data
    class Anthropic,OpenAI external
```

## Data Flow Examples

### 1. Intake Chat Flow (User ‚Üí Prediction)

```mermaid
sequenceDiagram
    participant User
    participant Frontend as Next.js Frontend
    participant API as FastAPI Backend
    participant IntakeService
    participant IntakeAgent
    participant Claude as Claude API
    participant Storage as File System

    User->>Frontend: Opens /chat
    Frontend->>API: POST /chat/start {role: "tenant"}
    API->>IntakeService: start_session("tenant")
    IntakeService->>IntakeAgent: start_conversation(role)
    IntakeAgent->>Claude: Generate greeting + first question
    Claude-->>IntakeAgent: Response
    IntakeAgent-->>IntakeService: Conversation state
    IntakeService->>Storage: Save session JSON
    IntakeService-->>API: Session data + greeting
    API-->>Frontend: {sessionId, response, stage, completeness}
    Frontend-->>User: Display chat interface

    loop Chat Conversation
        User->>Frontend: Sends message
        Frontend->>API: POST /chat/message {sessionId, message}
        API->>IntakeService: process_message(sessionId, message)
        IntakeService->>IntakeAgent: continue_conversation(message)
        IntakeAgent->>Claude: Extract entities + generate next question
        Claude-->>IntakeAgent: Extracted facts + response
        IntakeAgent-->>IntakeService: Updated case file + response
        IntakeService->>Storage: Update session JSON
        IntakeService-->>API: {response, completeness, isComplete}
        API-->>Frontend: Display response + progress
        Frontend-->>User: Show message + sidebar updates
    end

    IntakeService->>IntakeService: completeness >= 80%
    IntakeService->>Storage: Mark session complete
    IntakeService-->>Frontend: {isComplete: true, suggested_actions: ["generate_prediction"]}
    Frontend-->>User: Show "Generate Prediction" button
```

### 2. Prediction Generation Flow

```mermaid
sequenceDiagram
    participant User
    participant Frontend as Next.js Frontend
    participant API as FastAPI Backend
    participant PredictionService
    participant KGBuilder as Knowledge Graph
    participant RAG as RAG Pipeline
    participant Claude as Claude API
    participant Storage as File System

    User->>Frontend: Clicks "Generate Prediction"
    Frontend->>API: POST /predictions/generate {caseId}
    API->>PredictionService: generate_prediction(caseId)
    
    PredictionService->>Storage: Load case file
    Storage-->>PredictionService: Case JSON
    
    par Build Knowledge Graph
        PredictionService->>KGBuilder: build_graph(case_facts)
        KGBuilder->>KGBuilder: Extract entities + relationships
        KGBuilder->>Storage: Save KG JSON
        KGBuilder-->>PredictionService: Knowledge graph
    and Retrieve Similar Cases
        PredictionService->>RAG: retrieve_similar_cases(case_facts)
        RAG->>RAG: Hybrid search (BM25 + semantic)
        RAG->>RAG: Rerank by relevance
        RAG-->>PredictionService: Top 10 relevant cases
    end
    
    PredictionService->>Claude: Synthesize prediction with RAG + KG
    Note over Claude: Generates structured prediction:<br/>- Outcome (tenant_win/landlord_win/split)<br/>- Confidence scores<br/>- Reasoning trace with citations<br/>- Settlement ranges
    Claude-->>PredictionService: Structured prediction
    
    PredictionService->>Storage: Save prediction JSON
    PredictionService-->>API: Prediction result
    API-->>Frontend: Full prediction with reasoning
    Frontend-->>User: Display prediction page with:<br/>- Outcome & confidence<br/>- Reasoning trace<br/>- Similar cases<br/>- Settlement recommendations
```

### 3. Multi-Party Dispute Flow

```mermaid
sequenceDiagram
    participant Tenant
    participant TenantFE as Tenant's Browser
    participant API as FastAPI Backend
    participant DisputeService
    participant Storage as File System
    participant Landlord
    participant LandlordFE as Landlord's Browser

    Tenant->>TenantFE: Starts intake as "tenant"
    TenantFE->>API: POST /chat/start {role: "tenant", createDispute: true}
    API->>DisputeService: create_dispute()
    DisputeService->>Storage: Save dispute JSON
    DisputeService-->>API: {disputeId, inviteCode: "ABC123"}
    API-->>TenantFE: Session + Dispute info
    TenantFE-->>Tenant: Show invite code "ABC123"

    Tenant->>Tenant: Shares invite code with landlord
    
    Landlord->>LandlordFE: Opens app with invite code
    LandlordFE->>API: POST /disputes/validate-invite {inviteCode: "ABC123"}
    API->>DisputeService: validate_invite("ABC123")
    DisputeService->>Storage: Load dispute JSON
    DisputeService-->>API: {valid: true, expectedRole: "landlord"}
    API-->>LandlordFE: Validation success
    
    LandlordFE->>API: POST /chat/start {role: "landlord", inviteCode: "ABC123"}
    API->>DisputeService: join_dispute("ABC123", sessionId)
    DisputeService->>Storage: Update dispute with landlord session
    DisputeService-->>API: {joined: true, hasBothParties: true}
    API-->>LandlordFE: Start landlord intake
    
    Note over TenantFE,LandlordFE: Both parties complete their intakes independently
    
    TenantFE->>API: Check dispute status
    API->>DisputeService: get_dispute_status(disputeId)
    DisputeService-->>API: {bothPartiesComplete: true, readyForPrediction: true}
    API-->>TenantFE: Enable "Generate Joint Prediction"
    
    Tenant->>TenantFE: Generate prediction
    TenantFE->>API: POST /predictions/generate {caseId: disputeId}
    Note over API: Merges both parties' case files<br/>+ identifies conflicts<br/>+ weights evidence
    API-->>TenantFE: Combined prediction
    API-->>LandlordFE: Notify prediction available
```

## Key Architectural Patterns

### 1. **Separation of Concerns**
- **Frontend**: UI/UX, user interactions, client-side state
- **API Routers**: HTTP request handling, validation, response formatting
- **Services**: Business logic orchestration, session management
- **Packages**: Domain-specific logic (LLM, RAG, KG)

### 2. **Dependency Injection**
- Services use FastAPI's `Depends()` for singleton management
- Lazy loading of expensive resources (RAG pipeline)

### 3. **Async/Await Throughout**
- All API endpoints and services use `async`/`await`
- Non-blocking I/O for LLM calls, file operations
- Better resource utilization for concurrent users

### 4. **Structured Data with Pydantic**
- All API requests/responses use Pydantic models
- Type safety + automatic validation
- Consistent data contracts between frontend and backend

### 5. **Modular Package Design**
- Each package (`llm_orchestrator`, `rag_engine`, `kg_builder`) can be used independently
- Clear interfaces and minimal coupling
- Easy to test in isolation

### 6. **Cite-or-Abstain RAG Pattern**
- LLM predictions MUST cite retrieved cases
- No predictions without supporting evidence
- Transparency and legal defensibility

### 7. **Hybrid Search (BM25 + Semantic)**
- Combines keyword matching (BM25) with semantic similarity
- Better recall for legal terms + conceptual similarity
- Reranking stage for precision

### 8. **Knowledge Graph Validation**
- Constraint checking (temporal logic, evidence chains)
- Confidence scores on extracted facts
- Filters low-confidence data from predictions

## Technology Choices & Rationale

| Component | Technology | Why? |
|-----------|-----------|------|
| **Frontend** | Next.js 16 (App Router) | Server components, file-based routing, optimized bundle |
| **UI Library** | shadcn/ui + Tailwind | Modern, accessible, customizable components |
| **Backend** | FastAPI | Async support, automatic OpenAPI docs, Pydantic integration |
| **LLM** | Claude 3.5 Sonnet | Best reasoning for legal analysis, structured outputs |
| **Embeddings** | OpenAI text-embedding-3-large | High quality, 3072 dimensions, good for legal text |
| **Vector DB** | ChromaDB | Lightweight, embeddable, sufficient for 500 cases |
| **Graph DB** | JSON files (Neo4j future) | Simple for MVP, JSON is debuggable, Neo4j later for complex queries |
| **Storage** | Supabase | PostgreSQL + Auth + File storage in one, generous free tier |
| **Auth** | Supabase Auth | JWT-based, integrates with PostgreSQL RLS |

## Scaling Considerations

### Current MVP Design (500 cases, <100 users)
- ‚úÖ ChromaDB local (fast, simple)
- ‚úÖ JSON file storage (easy debugging)
- ‚úÖ In-memory session caching
- ‚úÖ Single FastAPI instance

### Future Scale (10K+ cases, 1000+ users)
- üîÑ Migrate ChromaDB to Pinecone/Weaviate (managed, distributed)
- üîÑ PostgreSQL for sessions, cases, predictions (ACID, relations)
- üîÑ Neo4j for knowledge graphs (complex queries, graph algorithms)
- üîÑ Redis for session caching (distributed, persistent)
- üîÑ Horizontal scaling with load balancer
- üîÑ Langfuse for LLM observability (cost tracking, latency, quality)

## Security & Compliance

### Data Protection
- PII redaction during RAG ingestion (hash names, addresses)
- Supabase RLS (Row-Level Security) for multi-tenancy
- HTTPS only, secure cookie handling

### Legal Safety
- All predictions include disclaimer: "This is not legal advice"
- Conditional language throughout ("likely", "similar cases suggest")
- Citation-backed claims only (no hallucination)

### Prompt Injection Defense
- Treat user input as untrusted
- System prompts enforce role boundaries
- Output validation (check citations exist)

## Development Workflow

### Local Development
```bash
# Terminal 1: Backend
cd apps/api
source ../../venv/bin/activate
python -m uvicorn src.main:app --reload --port 8000

# Terminal 2: Frontend
cd apps/web
npm run dev

# Access:
# Frontend: http://localhost:3000
# API Docs: http://localhost:8000/docs
```

### Testing Strategy
- **Unit Tests**: Individual packages (RAG, KG, LLM clients)
- **Integration Tests**: API endpoints with mocked LLM
- **Evaluation Tests**: Prediction accuracy on gold standard set
- **E2E Tests**: Full user flows (Playwright/Cypress)

## Monitoring & Observability

### Current (MVP)
- Structured logging with `structlog`
- Console output with color-coded log levels
- Manual inspection of JSON files

### Planned
- **Langfuse**: LLM call tracing, token usage, latency
- **Sentry**: Error tracking, performance monitoring
- **Posthog**: User analytics, feature usage
- **Grafana**: System metrics (CPU, memory, request rate)

---

## Learning Resources for Key Concepts

### What is RAG (Retrieval-Augmented Generation)?
RAG is a technique where we **retrieve relevant information from a knowledge base** (in our case, past tribunal decisions) **before generating a response**. Think of it like:
- You're writing an essay and you first look up relevant books/articles ‚Üí that's **Retrieval**
- Then you synthesize the information into your own words ‚Üí that's **Generation**
- **Why?** LLMs don't "know" specific tribunal decisions, but they're great at synthesizing information if we give them the right context.

### What is a Knowledge Graph?
A **Knowledge Graph (KG)** is a way to represent information as **nodes (entities) and edges (relationships)**:
- **Nodes**: Tenant, Landlord, Evidence (receipt), Claim (¬£500 for cleaning)
- **Edges**: "Receipt ‚Üí Supports ‚Üí Cleaning Claim", "Tenancy ‚Üí Ended ‚Üí 2024-01-15"
- **Why?** It helps us enforce logical consistency (e.g., "evidence dated after tenancy end can't support claim") and makes complex queries easier.

### What is Hybrid Search?
**Hybrid Search** combines two search methods:
1. **BM25 (keyword search)**: Like Ctrl+F, finds exact word matches (good for legal terms like "Section 21")
2. **Semantic Search (embeddings)**: Finds similar *meanings* even if words differ (e.g., "deposit protection" ‚âà "safeguarding scheme")
- **Why?** Legal text has both precise terminology (needs exact match) and conceptual similarity (needs semantic understanding)

### What are Embeddings?
**Embeddings** are numerical representations of text (like converting words to coordinates):
- "Tenant deposit dispute" ‚Üí `[0.23, -0.45, 0.67, ...]` (3072 numbers)
- Similar concepts have similar numbers (close in "semantic space")
- **Why?** Computers can't compare meanings directly, but they can compare numbers efficiently

### What is Async/Await?
**Async/await** lets our program do multiple things at once without blocking:
- **Sync (blocking)**: Make coffee ‚Üí Wait ‚Üí Make toast ‚Üí Wait ‚Üí Eat (8 minutes total)
- **Async (non-blocking)**: Start coffee ‚Üí Start toast (while coffee brews) ‚Üí Eat (5 minutes total)
- **Why?** LLM API calls take 2-5 seconds. With async, we can handle 10 users at once without each waiting for others.

### What is Structured Output?
**Structured Output** means the LLM returns data in a predictable format (like JSON):
```json
{
  "outcome": "tenant_win",
  "confidence": 0.85,
  "amount": 750.00
}
```
- **Why?** We can programmatically use this data (show progress bars, filter by confidence, etc.) instead of just displaying text.

---

**Questions?** Check the other docs:
- [API_DOCUMENTATION.md](./API_DOCUMENTATION.md) - Detailed API endpoint reference
- [USER_GUIDE.md](./USER_GUIDE.md) - How to use the system
- [DEBUG_LOGGING.md](./DEBUG_LOGGING.md) - Troubleshooting guide

